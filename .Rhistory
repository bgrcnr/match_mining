sets <- NULL
trainsets <- NULL
testsets <- NULL
for(i in 1:11)
{
trainsets[[i]] <- set3
testsets[[i]] <- set2
set1 <- set1 + 200
set3 <- c(set3,set1)
set2 <- set2 + 200
}
trainsets
set.seed(122)
k_levels=seq(10,125,by = 5)
nofReplications=10
nFolds=10
cvresult=data.table()
for(i in 1:11) {
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[testsets[[i]]], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Fold=j,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testsets[[i]]]))
}
}
trainsets[[i]]
testsets[[i]]
trainclass[testsets[[i]]]
cvtest
cvtrain
param_k
set.seed(122)
k_levels=seq(10,125,by = 5)
nofReplications=10
nFolds=10
indices=generateCVRuns(trainclass,nofReplications,nFolds,stratified=TRUE)
cvresult=data.table()
for(i in 1:11) {
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[testsets[[i]]], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Fold=j,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testsets[[i]]]))
}
}
cvtrain
sum(is.na(cvtrain))
i
for(i in 1:11) {
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[testsets[[i]]], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Fold=j,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testsets[[i]]]))
}
}
k
param_k
k <- NULL
for(i in 1:11) {
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[testsets[[i]]], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Fold=j,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testsets[[i]]]))
}
}
y
cvtest
#Seperate Results and Data, remove matchID, MatchDate and LeagueID columns
trainclass <- train_features$Match_Result
traindata <- train_features[,-c(1,2,6)]
testclass <- test_features$Match_Result
testdata <- test_features[,-c(1,2,6)]
#Results as numeric values
trainclass <- (trainclass == "Home")*1 + (trainclass == "Away")*2
testclass <- (testclass == "Home")*1 + (testclass == "Away")*2
train_features=features[Match_Date>=trainStart & Match_Date<testStart]
test_features=features[Match_Date>=testStart]
#keep complete cases
train_features <- train_features[complete.cases(train_features)]
test_features <- test_features[complete.cases(test_features)]
#Seperate Results and Data, remove matchID, MatchDate and LeagueID columns
trainclass <- train_features$Match_Result
traindata <- train_features[,-c(1,2,6)]
testclass <- test_features$Match_Result
testdata <- test_features[,-c(1,2,6)]
#Results as numeric values
trainclass <- (trainclass == "Home")*1 + (trainclass == "Away")*2
testclass <- (testclass == "Home")*1 + (testclass == "Away")*2
testsets
k_levels
cvresult=data.table()
i = 1
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtrain
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
cvtest
param_k=k_levels[y]
param_k
predict_knn=knn(cvtrain, cvtest,trainclass[testsets[[i]]], k = param_k)
trainclass[testsets[[i]]]
predict_knn=knn(cvtrain, cvtest,trainclass[trainsets[[i]]], k = param_k)
trainsets[[i]]]
trainclass[trainsets[[i]]]
predict_knn=knn(cvtrain, cvtest,trainclass[trainsets[[i]]], k = param_k)
?knn
trainclass
testindices
for(i in 1:11) {
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Fold=j,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testindices]))
}
}
testindices
trainindices
param_k
cvtest
trainclass[trainindices]
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
traindata
ncol(traindata)
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
ncol(cvtrain)
ncol(cvtest)
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
nrow(cvtrain)
length(trainindices)
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
trainclass[trainindices]
predict_knn=knn(cvtrain, cvtest,as.factor(trainclass[trainindices]), k = param_k)
trainclass[trainindices]
is.na(trainclass[trainindices])
require(data.table)
require(anytime)
require(scatterplot3d)
require(FNN)
require(glmnet)
require(TunePareto)
require(data.table)
require(RANN.L1)
require(lubridate)
require(devtools)
require(nnet)
library(adabag)
require(pracma)
require(gbm)
require(xgboost)
require(e1071)
require(Ckmeans.1d.dp)
source('data_preprocessing.r')
source('feature_extraction.r')
source('performance_metrics.r')
source('train_models.r')
### match_processing is for adding average goals, average scores and days before the match
source('match_processing.r')
#save paths
matches_data_path = "Files/df9b1196-e3cf-4cc7-9159-f236fe738215_matches.rds"
odd_details_data_path = "Files/df9b1196-e3cf-4cc7-9159-f236fe738215_odd_details.rds"
#train and test dates
testStart=as.Date('2018-03-16')
trainStart=as.Date('2011-09-15')
rem_miss_threshold=0.01 #parameter for removing bookmaker odds with missing ratio greater than this threshold
# read data
city_distances <- read.csv("city_distances.csv")
s_2018 <- as.data.table(read.csv("Files/2018_data.csv"))
s_2017 <- as.data.table(read.csv("Files/2017_data.csv"))
s_2016 <- as.data.table(read.csv("Files/2016_data.csv"))
s_2015 <- as.data.table(read.csv("Files/2015_data.csv"))
s_2014 <- as.data.table(read.csv("Files/2014_data.csv"))
s_2013 <- as.data.table(read.csv("Files/2013_data.csv"))
s_2012 <- as.data.table(read.csv("Files/2012_data.csv"))
s_2011 <- as.data.table(read.csv("Files/2011_data.csv"))
s_2010 <- as.data.table(read.csv("Files/2010_data.csv"))
matches_raw=readRDS(matches_data_path)
odd_details_raw=readRDS(odd_details_data_path)
# preprocess matches
#matched datapreprocessing function is edited, it adds unixdate and weekday columns
matches=matches_data_preprocessing(matches_raw)
additional_data <- rbind(s_2010[,1:23],s_2011[,1:23],s_2012[,1:23],s_2013[,1:23],s_2014[,1:23]
,s_2015[,1:23],s_2016[,1:23],s_2017[,1:23],s_2018[,1:23])
additional_data$HomeTeam <- as.character(additional_data$HomeTeam)
additional_data$AwayTeam <- as.character(additional_data$AwayTeam)
additional_data$Date <- dmy(additional_data$Date)
additional_data <- additional_data[complete.cases(additional_data)]
names1 <- sort(unique(additional_data$HomeTeam))
names2 <- sort(unique(matches$Home))
names3 <- cbind(names1,names2)
for(i in 1:nrow(names3))
{
additional_data[HomeTeam == names3[i,1]]$HomeTeam <- names3[i,2]
additional_data[AwayTeam == names3[i,1]]$AwayTeam <- names3[i,2]
}
additional_data <- additional_data[,Div:=NULL]
col_names <- c("Date", "Home", "Away", "Full Time Home Team Goals",
"Full Time Away Team Goals", "Full Time Result" ,"Half Time Home Team Goals",
"Half Time Away Team Goals", "Half Time Result", "Referee", "Home Team Shots",
"Away Team Shots", "Home Team Shots on Target", "Away Team Shots on Target",
"Home Team Fouls Committed", "Away Team Fouls Committed", "Home Team Corners",
"Away Team Corners", "Home Team Yellow Cards", "Away Team Yellow Cards",
"Home Team Red Cards", "Away Team Red Cards")
colnames(additional_data) <- col_names
comp_data <- merge(matches, additional_data, by.x = c("Match_Date", "Home", "Away"), by.y  = c("Date", "Home", "Away"), all.x = TRUE)
## Add extra features to matches (winning average, score average, days before the match)
matches <- match_processing(comp_data, 3, "t")
# preprocess odd data
odd_details=details_data_preprocessing(odd_details_raw,matches,which_bets = c("1x2"))
# extract open and close odd type features from multiple bookmakers
features=extract_features.openclose(matches,odd_details,pMissThreshold=rem_miss_threshold,trainStart,testStart)
# divide data based on the provided dates
train_features=features[Match_Date>=trainStart & Match_Date<testStart]
test_features=features[Match_Date>=testStart]
#keep complete cases
train_features <- train_features[complete.cases(train_features)]
test_features <- test_features[complete.cases(test_features)]
#Seperate Results and Data, remove matchID, MatchDate and LeagueID columns
trainclass <- train_features$Match_Result
traindata <- train_features[,-c(1,2,6)]
testclass <- test_features$Match_Result
testdata <- test_features[,-c(1,2,6)]
#Results as numeric values
trainclass <- (trainclass == "Home")*1 + (trainclass == "Away")*2
testclass <- (testclass == "Home")*1 + (testclass == "Away")*2
set1 <- 1:200
set3 <- 1:200
set2 <- 201:210
nrow(traindata)
sets <- NULL
trainsets <- NULL
testsets <- NULL
for(i in 1:11)
{
trainsets[[i]] <- set3
testsets[[i]] <- set2
set1 <- set1 + 200
set3 <- c(set3,set1)
set2 <- set2 + 200
}
trainsets
set.seed(122)
k_levels=seq(10,125,by = 5)
nofReplications=10
nFolds=10
indices=generateCVRuns(trainclass,nofReplications,nFolds,stratified=TRUE)
cvresult=data.table()
for(i in 1:11) {
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Fold=j,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testindices]))
}
}
traindata[trainsets[[i]],]
traindata[testsets[[i]],]
cvtrain=as.data.table(scale(traindata[trainsets[[i]],]))
cvtest=as.data.table(scale(traindata[testsets[[i]],]))
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
warnings()
knn
any(is.na(cvtrain))
any(is.na(cvtest))
is.na(cvtest)
sum(is.na(cvtest))
sum(is.na(traindata))
sum(is.na(traindata[testsets[[i]],]))
sum(is.na(scale(traindata[testsets[[i]],])))
traindata[testsets[[i]],]$Home_Last_Red
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=as.data.table(scale(traindata[trainindices,]))
cvtest=as.data.table(scale(traindata[testindices,]))
cvtrain[is.na(cvtrain)] <- 0
cvtest[is.na(cvtest)] <- 0
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Fold=j,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testindices]))
}
}
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=as.data.table(scale(traindata[trainindices,]))
cvtest=as.data.table(scale(traindata[testindices,]))
cvtrain[is.na(cvtrain)] <- 0
cvtest[is.na(cvtest)] <- 0
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testindices]))
}
}
comparison <- cvresult[,list(Accu=mean(Predictions==Real)),by=list(Method,Klev)]
comparison <- comparison[order(Accu)]
comparison
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=traindata[trainindices,]
cvtest=traindata[testindices,]
cvtrain = cbind(trainclass[trainindices],cvtrain)
#Model is generated
multinomModel <- multinom(V1 ~.,data= cvtrain)
#Probabilities for each class and the results are generated
predicted_scores <- predict (multinomModel, cvtest, "probs") # predict on new data
predicted_class <- predict (multinomModel, cvtest)
#Confusion matrix and accuracy
confusion_matrix_multinom <- table(predicted_class, trainclass[testindices])
accuracy_multinom <- sum(predicted_class == trainclass[testindices])/length(trainclass[testindices])
cvresult=rbind(cvresult,data.table(Replication=i,Method='multinom',TestId=testindices,
Predictions=as.numeric(as.character(predicted_class)),Real=trainclass[testindices]))
}
trainclass[trainindices]
cvtrain
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=traindata[trainindices,]
cvtest=traindata[testindices,]
cvtrain = cbind(trainclass[trainindices],cvtrain)
#Model is generated
multinomModel <- multinom(V1 ~.,data= cvtrain)
#Probabilities for each class and the results are generated
predicted_scores <- predict (multinomModel, cvtest, "probs") # predict on new data
predicted_class <- predict (multinomModel, cvtest)
#Confusion matrix and accuracy
confusion_matrix_multinom <- table(predicted_class, trainclass[testindices])
accuracy_multinom <- sum(predicted_class == trainclass[testindices])/length(trainclass[testindices])
cvresult=rbind(cvresult,data.table(Replication=i,Method='multinom',TestId=testindices,
Predictions=as.numeric(as.character(predicted_class)),Real=trainclass[testindices]))
}
comparison <- cvresult[,list(Accu=mean(Predictions==Real)),by=list(Method)]
comparison <- comparison[order(Accu)]
comparison
#Model is generated
multinomModel <- multinom(V1 ~.,data= cvtrain)
#Probabilities for each class and the results are generated
predicted_scores <- predict (multinomModel, cvtest, "probs") # predict on new data
predicted_class <- predict (multinomModel, cvtest)
#Confusion matrix and accuracy
confusion_matrix_multinom <- table(predicted_class, trainclass[testindices])
accuracy_multinom <- sum(predicted_class == trainclass[testindices])/length(trainclass[testindices])
accuracy_multinom
cvresult=data.table()
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=traindata[trainindices,]
cvtest=traindata[testindices,]
cvtrain = cbind(trainclass[trainindices],cvtrain)
#Model is generated
multinomModel <- multinom(V1 ~.,data= cvtrain)
#Probabilities for each class and the results are generated
predicted_scores <- predict (multinomModel, cvtest, "probs") # predict on new data
predicted_class <- predict (multinomModel, cvtest)
#Confusion matrix and accuracy
confusion_matrix_multinom <- table(predicted_class, trainclass[testindices])
accuracy_multinom <- sum(predicted_class == trainclass[testindices])/length(trainclass[testindices])
cvresult=rbind(cvresult,data.table(Replication=i,Method='multinom',TestId=testindices,
Predictions=as.numeric(as.character(predicted_class)),Real=trainclass[testindices]))
}
comparison <- cvresult[,list(Accu=mean(Predictions==Real)),by=list(Method)]
comparison <- comparison[order(Accu)]
comparison
set1 <- 1:200
set3 <- 1:200
set2 <- 201:250
sets <- NULL
trainsets <- NULL
testsets <- NULL
for(i in 1:11)
{
trainsets[[i]] <- set3
testsets[[i]] <- set2
set1 <- set1 + 200
set3 <- c(set3,set1)
set2 <- set2 + 200
}
set.seed(250)
k_levels=c(1:10)
nofReplications=10
nFolds=10
indices=generateCVRuns(trainclass,nofReplications,nFolds,stratified=TRUE)
cvresult=data.table()
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=traindata[trainindices,]
cvtest=traindata[testindices,]
cvtrain = cbind(trainclass[trainindices],cvtrain)
#Model is generated
multinomModel <- multinom(V1 ~.,data= cvtrain)
#Probabilities for each class and the results are generated
predicted_scores <- predict (multinomModel, cvtest, "probs") # predict on new data
predicted_class <- predict (multinomModel, cvtest)
#Confusion matrix and accuracy
confusion_matrix_multinom <- table(predicted_class, trainclass[testindices])
accuracy_multinom <- sum(predicted_class == trainclass[testindices])/length(trainclass[testindices])
cvresult=rbind(cvresult,data.table(Replication=i,Method='multinom',TestId=testindices,
Predictions=as.numeric(as.character(predicted_class)),Real=trainclass[testindices]))
}
comparison <- cvresult[,list(Accu=mean(Predictions==Real)),by=list(Method)]
comparison <- comparison[order(Accu)]
comparison
set.seed(122)
k_levels=seq(10,125,by = 5)
nofReplications=10
nFolds=10
indices=generateCVRuns(trainclass,nofReplications,nFolds,stratified=TRUE)
cvresult=data.table()
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=as.data.table(scale(traindata[trainindices,]))
cvtest=as.data.table(scale(traindata[testindices,]))
cvtrain[is.na(cvtrain)] <- 0
cvtest[is.na(cvtest)] <- 0
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testindices]))
}
}
comparison <- cvresult[,list(Accu=mean(Predictions==Real)),by=list(Method,Klev)]
comparison <- comparison[order(Accu)]
comparison
# Inputs are generated from data files
cols <- names(traindata)
cols
cbind(cols, 1:length(cols))
cols <- names(traindata)[c(5:16,23:30,37:40,55,70,85,100,115)]
cols
traindata <- traindata[,..cols]
testdata <- testdata[,..cols]
set.seed(250)
k_levels=c(1:10)
nofReplications=10
nFolds=10
indices=generateCVRuns(trainclass,nofReplications,nFolds,stratified=TRUE)
cvresult=data.table()
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=traindata[trainindices,]
cvtest=traindata[testindices,]
cvtrain = cbind(trainclass[trainindices],cvtrain)
#Model is generated
multinomModel <- multinom(V1 ~.,data= cvtrain)
#Probabilities for each class and the results are generated
predicted_scores <- predict (multinomModel, cvtest, "probs") # predict on new data
predicted_class <- predict (multinomModel, cvtest)
#Confusion matrix and accuracy
confusion_matrix_multinom <- table(predicted_class, trainclass[testindices])
accuracy_multinom <- sum(predicted_class == trainclass[testindices])/length(trainclass[testindices])
cvresult=rbind(cvresult,data.table(Replication=i,Method='multinom',TestId=testindices,
Predictions=as.numeric(as.character(predicted_class)),Real=trainclass[testindices]))
}
comparison <- cvresult[,list(Accu=mean(Predictions==Real)),by=list(Method)]
comparison <- comparison[order(Accu)]
comparison
set.seed(122)
k_levels=seq(10,125,by = 5)
nofReplications=10
nFolds=10
indices=generateCVRuns(trainclass,nofReplications,nFolds,stratified=TRUE)
cvresult=data.table()
for(i in 1:11) {
testindices <- testsets[[i]]
trainindices <- trainsets[[i]]
cvtrain=as.data.table(scale(traindata[trainindices,]))
cvtest=as.data.table(scale(traindata[testindices,]))
cvtrain[is.na(cvtrain)] <- 0
cvtest[is.na(cvtest)] <- 0
for(y in 1:length(k_levels)){
param_k=k_levels[y]
predict_knn=knn(cvtrain, cvtest,trainclass[trainindices], k = param_k)
cvresult=rbind(cvresult,data.table(Replication=i,Method='knn',Klev=param_k,TestId=testindices,
Predictions=as.numeric(as.character(predict_knn)),Real=trainclass[testindices]))
}
}
comparison <- cvresult[,list(Accu=mean(Predictions==Real)),by=list(Method,Klev)]
comparison <- comparison[order(Accu)]
comparison
